<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>EIT NLP - 首页</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <header>
        <div class="header-content">
            <nav>
                <ul>
                    <li><h1>EIT NLP</h1></li>
                    <li><a href="index-zh.html">首页</a></li>
                    <li><a href="research-zh.html">研究</a></li>
                    <li><a href="team-zh.html">团队</a></li>
                    <li><a href="recruitment-zh.html">招聘</a></li>
                </ul>
            </nav>
            <div class="language-switch">
                <a href="index.html">English</a>
            </div>
        </div>
    </header>

    <main>
        <div class="hero-image">
            <div class="hero-text">
                <h2>欢迎来到东方理工大学（宁波）自然语言处理组！</h2>
            </div>
            <a href="https://www.eitech.edu.cn/" target="_blank">了解更多</a>
        </div>

        <section>
            <h2>关于我们</h2>
            <p class="about-us-text">我们的研究探索信息检索、多模态学习和思维链推理等令人兴奋的领域。我们的一个关键重点是创建高效算法，在最小化资源使用的同时提供卓越的性能。我们致力于减少对大量人工标注的依赖，简化训练时间，并降低推理成本。最终，我们的目标是构建在各种任务中表现出色的模型，同时注重资源效率并对所有人都可访问。加入我们，一起重塑NLP的未来！</p>
        </section>

        <section class="news">
            <h2>新闻</h2>
            <ul>
                <li>
                    <strong>New Paper Released:</strong> "To Preserve or To Compress: An In-Depth Study of Connector Selection in Multimodal Large Language Models"
                    <br>
                    Authors: Junyan Lin, Haoran Chen, Dawei Zhu, Xiaoyu Shen
                    <br>
                    <a href="http://arxiv.org/abs/2410.06765" target="_blank">Paper</a> | 
                    <a href="https://github.com/EIT-NLP/Connector-Selection-for-MLLM" target="_blank">Code Repository</a>
                    <br>
                    传统上，多语言机器翻译的成功归因于训练数据的三个关键因素：大规模数据量、多样化的翻译方向以及高质量的数据。在当前微调大语言模型（LLMs）以进行翻译的实践中，我们重新审视了这些因素的重要性。我们发现，LLMs 在仅微调 32 对平行句子的情况下就表现出了强大的翻译能力，并且微调单一翻译方向能够实现多方向的翻译。然而，方向的选择至关重要：仅使用英文作为目标语言进行微调可能导致任务误解，从而阻碍向非英语语言的翻译。当噪声合成数据作为目标语言时，特别是当目标语言在 LLM 预训练中已得到良好表示的情况下，也会出现问题。然而，值得注意的是，合成数据对一个在预训练中代表性较低的语言的影响较为轻微。我们的研究结果表明，在将 LLMs 适配于翻译任务时，对数据量的要求可以适当放宽，但仍需仔细考虑，以防止 LLM 利用非预期的数据偏差。
                </li>
                <li>
                    <strong>Research Highlight:</strong> "Fine-Tuning Large Language Models to Translate: Will a Touch of Noisy Data in Misaligned Languages Suffice?"
                    <br>
                    Authors: Dawei Zhu, Pinzhen Chen, Miaoran Zhang, Barry Haddow, Xiaoyu Shen, Dietrich Klakow
                    <br>
                    <a href="https://arxiv.org/pdf/2404.14122" target="_blank">Paper</a> | 
                    <a href="https://github.com/uds-lsv/mt-sft" target="_blank">Code Repository</a>
                    <br>
                    This study revisits the importance of training data factors in fine-tuning LLMs for translation, revealing surprising findings about data volume and translation direction requirements.
                </li>
                <li>
                    <strong>Latest Publication:</strong> "The Accuracy Paradox in RLHF: When Better Reward Models Don't Yield Better Language Models"
                    <br>
                    Authors: Yanjun Chen, Dawei Zhu, Yirong Sun, Xinghao Chen, Wei Zhang, Xiaoyu Shen
                    <br>
                    <a href="https://arxiv.org/abs/2410.06554" target="_blank">Paper</a> | 
                    <a href="https://github.com/EIT-NLP/AccuracyParadox-RLHF" target="_blank">Code Repository</a>
                    <br>
                    This research reveals a surprising paradox in human feedback reinforcement learning, challenging the belief that stronger reward models always lead to better language model performance.
                </li>
            </ul>
            </ul>
            <a href="research-zh.html" class="see-more">查看更多</a>
        </section>

        <section>
            <h2>Github</h2>
            <p>我们实验室专注于开放科学，并将在以下地址发布我们的代码、模型和数据集 <a href="https://github.com/eit-nlp" target="_blank" class="button-link">访问我们的GitHub</a></p>
        </section>
    </main>

    <footer>
        <p>&copy; 2023 EIT NLP. 保留所有权利。</p>
    </footer>

    <script src="script.js"></script>
</body>
</html>
