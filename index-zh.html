<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>EIT NLP - 首页</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <header>
        <div class="header-content">
            <nav>
                <ul>
                    <li><h1>EIT NLP</h1></li>
                    <li><a href="index-zh.html">首页</a></li>
                    <li><a href="research-zh.html">研究</a></li>
                    <li><a href="team-zh.html">团队</a></li>
                    <li><a href="recruitment-zh.html">招聘</a></li>
                </ul>
            </nav>
            <div class="language-switch">
                <a href="index.html">English</a>
            </div>
        </div>
    </header>

    <main>
        <div class="hero-image">
            <div class="hero-text">
                <h2>欢迎来到宁波东方理工学院自然语言处理组！</h2>
            </div>
            <a href="https://www.eitech.edu.cn/" target="_blank">了解更多</a>
        </div>

        <section>
            <h2>关于我们</h2>
            <p class="about-us-text">我们的研究探索信息检索、多模态学习和思维链推理等令人兴奋的领域。我们的一个关键重点是创建高效算法，在最小化资源使用的同时提供卓越的性能。我们致力于减少对大量人工标注的依赖，简化训练时间，并降低推理成本。最终，我们的目标是构建在各种任务中表现出色的模型，同时注重资源效率并对所有人都可访问。加入我们，一起重塑NLP的未来！</p>
        </section>

        <section class="news">
            <h2>新闻</h2>
            <ul>
                <li>
                    <strong>新论文发布：</strong> "保留还是压缩：多模态大型语言模型连接器选择的深入研究"
                    <br>
                    作者：林俊彦、陈浩然、朱大卫、沈晓宇
                    <br>
                    <a href="http://arxiv.org/abs/2410.06765" target="_blank">论文</a> | 
                    <a href="https://github.com/EIT-NLP/Connector-Selection-for-MLLM" target="_blank">代码仓库</a>
                    <br>
                    本研究探讨了连接器对多模态大型语言模型（MLLMs）性能的影响，为MLLM架构设计提供了重要指导。
                </li>
                <li>
                    <strong>研究亮点：</strong> "微调大型语言模型进行翻译：少量不对齐语言的噪声数据是否足够？"
                    <br>
                    作者：朱大卫、陈品真、张妙然、Barry Haddow、沈晓宇、Dietrich Klakow
                    <br>
                    <a href="https://arxiv.org/pdf/2404.14122" target="_blank">论文</a> | 
                    <a href="https://github.com/uds-lsv/mt-sft" target="_blank">代码仓库</a>
                    <br>
                    本研究重新审视了在微调LLMs进行翻译时训练数据因素的重要性，揭示了关于数据量和翻译方向要求的令人惊讶的发现。
                </li>
                <li>
                    <strong>最新发表：</strong> "RLHF中的准确性悖论：为什么更好的奖励模型不一定产生更好的语言模型"
                    <br>
                    作者：陈彦君、朱大卫、孙一荣、陈星皓、张伟、沈晓宇
                    <br>
                    <a href="https://arxiv.org/abs/2410.06554" target="_blank">论文</a> | 
                    <a href="https://github.com/EIT-NLP/AccuracyParadox-RLHF" target="_blank">代码仓库</a>
                    <br>
                    这项研究揭示了人类反馈强化学习中的一个令人惊讶的悖论，挑战了更强的奖励模型总是会导致更好的语言模型性能的普遍信念。
                </li>
            </ul>
            <a href="research-zh.html" class="see-more">查看更多</a>
        </section>

        <section>
            <h2>Github</h2>
            <p>我们实验室专注于开放科学，并将在以下地址发布我们的代码、模型和数据集 <a href="https://github.com/eit-nlp" target="_blank" class="button-link">访问我们的GitHub</a></p>
        </section>
    </main>

    <footer>
        <p>&copy; 2023 EIT NLP. 保留所有权利。</p>
    </footer>

    <script src="script.js"></script>
</body>
</html>
