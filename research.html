<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>EIT NLP - Research</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <header>
        <div class="header-content">
            <nav>
                <ul>
                    <li><h1>EIT NLP</h1></li>
                    <li><a href="index.html">Home</a></li>
                    <li><a href="research.html">Research</a></li>
                    <li><a href="team.html">Team</a></li>
                    <li><a href="recruitment.html">Recruitment</a></li>
                </ul>
            </nav>
        </div>
    </header>

    <main>
        <h1>Publications</h1>

        <div class="publication-container">
            <div class="publication">
                <img src="path/to/image1.jpg" alt="Fine-Tuning Large Language Models to Translate">
                <div class="publication-content">
                    <h2>Fine-Tuning Large Language Models to Translate: Will a Touch of Noisy Data in Misaligned Languages Suffice?</h2>
                    <p><strong>Authors:</strong> Dawei Zhu, Pinzhen Chen, Miaoran Zhang, Barry Haddow, Xiaoyu Shen, Dietrich Klakow</p>
                    <p><strong>Paper address:</strong> <a href="https://arxiv.org/pdf/2404.14122" target="_blank">https://arxiv.org/pdf/2404.14122</a></p>
                    <p><strong>Code:</strong> <a href="https://github.com/uds-lsv/mt-sft" target="_blank">https://github.com/uds-lsv/mt-sft</a></p>
                    <p><strong>Paper Description:</strong> Traditionally, the success of multilingual machine translation has been attributed to three key factors of training data: large-scale data volume, diverse translation directions, and high quality data. In the current practice of fine-tuning Large Language Models (LLMs) for translation, we revisit the importance of these factors. We find that LLMs exhibit strong translation capabilities when fine-tuned with only 32 pairs of parallel sentences, and that fine-tuning a single translation direction enables multi-directional translation. However, the choice of direction is crucial: fine-tuning using only English as the target language may lead to task misinterpretation, thus hindering translation to non-English languages. Problems can also arise when noisy synthetic data is used as the target language, especially when the target language has been well represented in LLM pre-training. However, it is worth noting that the effect of synthetic data on a language that is poorly represented in the pre-training is milder. Our results suggest that the data volume requirement can be appropriately relaxed when adapting LLMs to translation tasks, but careful consideration is still needed to prevent LLMs from utilizing unintended data biases.</p>
                </div>
            </div>

            <div class="publication reverse">
                <img src="path/to/image2.jpg" alt="The Accuracy Paradox in RLHF">
                <div class="publication-content">
                    <h2>The Accuracy Paradox in RLHF: When Better Reward Models Don't Yield Better Language Models</h2>
                    <p><strong>Author(s):</strong> Yanjun Chen, Dawei Zhu, Yirong Sun, Xinghao Chen, Wei Zhang, Xiaoyu Shen</p>
                    <p><strong>Address:</strong> <a href="https://arxiv.org/abs/2410.06554" target="_blank">https://arxiv.org/abs/2410.06554</a></p>
                    <p><strong>Code:</strong> <a href="https://github.com/EIT-NLP/AccuracyParadox-RLHF" target="_blank">https://github.com/EIT-NLP/AccuracyParadox-RLHF</a></p>
                    <p><strong>Paper Description:</strong> Human feedback reinforcement learning significantly improves natural language processing by training language models to better match human expectations. The strength of the reward model is one of the key factors in this process. This study examines whether stronger reward models necessarily lead to better language model performance. Using experiments with the QA-FEEDBACK dataset and Longformer-based reward models for relevance, factuality, and completeness tasks, this paper reveals a surprising paradox: language models trained with medium-accuracy reward models outperform those trained with high-accuracy reward models. This finding challenges the common belief that stronger reward models always lead to better language model performance, and opens up new directions for future research on the key factors affecting model performance and how to choose appropriate reward models.</p>
                </div>
            </div>

            <div class="publication">
                <img src="path/to/image3.jpg" alt="Unveiling In-Context Learning">
                <div class="publication-content">
                    <h2>Unveiling In-Context Learning: A Coordinate System to Understand Its Working Mechanism</h2>
                    <p><strong>Author(s):</strong> Anhao Zhao, Fanghua Ye, Jinlan Fu, Xiaoyu Shen</p>
                    <p><strong>Address:</strong> <a href="https://arxiv.org/pdf/2407.17011" target="_blank">https://arxiv.org/pdf/2407.17011</a></p>
                    <p><strong>Code:</strong> <a href="https://github.com/EIT-NLP/2D-Coordinate-System-for-ICL" target="_blank">https://github.com/EIT-NLP/2D-Coordinate-System-for-ICL</a></p>
                    <p><strong>Abstract:</strong> Large Language Models (LLMs) have demonstrated a remarkable capability of Context Learning (ICL), however, the working mechanism behind it is still not fully understood. Existing studies have presented two conflicting views on ICL: one emphasizes the importance of similar samples in the demonstration, pointing out the importance of labeling correctness and more examples; the other attributes ICL to the inherent task recognition ability of LLMs, arguing that labeling correctness and the number of examples are not critical. In this study, we propose a two-dimensional coordinate system that unifies these two views in a systematic framework. The framework explains ICL behavior through two orthogonal variables: whether the presentations contain similar samples (perception) and whether the LLMs are able to recognize the task (cognition). We propose peak inverse-order ranking metrics for detecting LLMs' task recognition ability and investigate LLMs' responses to different definitions of similarity. Based on this, we conducted extensive experiments to elucidate the performance of ICL on several representative categorization tasks. Finally, we extend our analysis to generative tasks, showing that this coordinate system is equally effective in explaining the mechanisms by which ICL works in generative tasks.</p>
                </div>
            </div>

            <div class="publication reverse">
                <img src="path/to/image4.jpg" alt="To Preserve or To Compress">
                <div class="publication-content">
                    <h2>To Preserve or To Compress: An In-Depth Study of Connector Selection in Multimodal Large Language Models</h2>
                    <p><strong>Author(s):</strong> Junyan Lin, Haoran Chen, Dawei Zhu, Xiaoyu Shen</p>
                    <p><strong>Address:</strong> <a href="http://arxiv.org/abs/2410.06765" target="_blank">http://arxiv.org/abs/2410.06765</a></p>
                    <p><strong>Code:</strong> <a href="https://github.com/EIT-NLP/Connector-Selection-for-MLLM" target="_blank">https://github.com/EIT-NLP/Connector-Selection-for-MLLM</a></p>
                    <p><strong>Paper Introduction:</strong> In recent years, Multimodal Large Language Models (MLLMs) have attracted a lot of attention from industry and academia. Depending on the fusion location, MLLMs can be categorized into external fusion and internal fusion architectures, among which external fusion architectures are dominant. However, there is still a big controversy about how to build optimal external fusion MLLM architectures, especially the performance of different connectors in different granularity tasks. In this paper, we systematically explore the impact of connectors on MLLM performance. Specifically, we categorize connectors into two types: feature-preserving and feature-compressing. With a unified classification criterion, we classify subtasks from three comprehensive benchmark datasets (MMBench, MME, SEED-Bench) into three task types: coarse-grained perception, fine-grained perception, and inference, and evaluate the performance of connectors from this perspective. The results show that the performance of different connector types varies significantly across tasks, which provides important guidance for MLLM architecture design and advances the understanding and development of MLLM architecture optimization.</p>
                </div>
            </div>

            <div class="publication">
                <img src="path/to/image5.jpg" alt="Deeper Insights Without Updates">
                <div class="publication-content">
                    <h2>Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning</h2>
                    <p><strong>Author(s):</strong> Qingyu Yin, Xuzheng He, Luoao Deng, Chak Tou Leong, Fan Wang, Yanzhao Yan, Xiaoyu Shen, Qiang Zhang</p>
                    <p><strong>Address:</strong> <a href="https://arxiv.org/abs/2410.04691" target="_blank">https://arxiv.org/abs/2410.04691</a></p>
                    <p><strong>Code:</strong> <a href="https://github.com/MikaStars39/ICLvsFinetune" target="_blank">https://github.com/MikaStars39/ICLvsFinetune</a></p>
                    <p><strong>Thesis Statement:</strong> Fine-tuning and In-Context Learning (ICL) are two commonly used methods for injecting task-specific knowledge into large language models. It is often assumed that fine-tuning outperforms ICL with sufficient samples due to its ability to tune the internal parameters of the model based on training data; however, this paper presents a counter-intuitive finding: for implicit pattern tasks, ICL can significantly outperform fine-tuning in capturing these patterns. We constructed multiple datasets containing implicit patterns, such as sequences of answers determined by parity or identifying approximable terms in a computation. We then evaluated the model's understanding of these patterns under fine-tuning and ICL conditions on models with parameter scales ranging from 0.5B to 7B, respectively. The results show that models using ICL can quickly grasp deep patterns and significantly improve accuracy. In contrast, fine-tuning provides limited performance gains despite using thousands of times more training samples than ICL. We also propose a theory of circuit migration from the perspective of mechanical interpretability that explains why ICL performs better on such tasks.</p>
                </div>
            </div>

            <div class="publication reverse">
                <img src="path/to/image6.jpg" alt="LawBench: Benchmarking Legal Knowledge of Large Language Models">
                <div class="publication-content">
                    <h2>LawBench: Benchmarking Legal Knowledge of Large Language Models</h2>
                    <p><strong>Authors:</strong> Zhiwei Fei, Xiaoyu Shen, Dawei Zhu, Fengzhe Zhou, Zhuo Han, Alan Huang, Songyang Zhang, Kai Chen, Zhixin Yin, Zongwen Shen, Jidong Ge, Vincent Ng</p>
                    <p><strong>Paper address:</strong> <a href="https://arxiv.org/pdf/2309.16289" target="_blank">https://arxiv.org/pdf/2309.16289</a></p>
                    <p><strong>Code:</strong> <a href="https://github.com/open-compass/LawBench/" target="_blank">https://github.com/open-compass/LawBench/</a></p>
                    <p><strong>Dissertation Synopsis:</strong> We present LawBench, the first assessment benchmark consisting of 20 tasks designed to evaluate the performance of Large Language Models (LLMs) in processing Chinese law-related tasks. LawBench has been carefully designed to accurately assess the legal competence of LLMs in terms of three cognitive levels corresponding to the widely-accepted Bloom's Cognitive Taxonomy. Using LawBench, we provide a comprehensive assessment of 21 popular LLMs and, for the first time, a comparative analysis of their actual performance, revealing their relative strengths and weaknesses. All data, model predictions, and evaluation code are available from <a href="https://github.com/open-compass/LawBench" target="_blank">https://github.com/open-compass/LawBench</a>.</p>
                </div>
            </div>

            <div class="publication">
                <img src="path/to/image7.jpg" alt="Assessing Implicit Retrieval Robustness of Large Language Models">
                <div class="publication-content">
                    <h2>Assessing "Implicit" Retrieval Robustness of Large Language Models</h2>
                    <p><strong>Author(s):</strong> Xiaoyu Shen, Rexhina Blloshmi, Dawei Zhu, Jiahuan Pei, Wei Zhang</p>
                    <p><strong>Address:</strong> <a href="https://arxiv.org/pdf/2406.18134" target="_blank">https://arxiv.org/pdf/2406.18134</a></p>
                    <p><strong>Paper Introduction:</strong> Retrieval-augmented generation (RAG) has attracted much attention in recent years as a framework for augmenting large language models with external knowledge. However, its effectiveness relies heavily on the retrieval robustness of the model. If the model lacks retrieval robustness, its performance will be limited by the accuracy of the retriever, and when the retrieved context is task-irrelevant, the model performance may drop dramatically. In this paper, we evaluate the "implicit" retrieval robustness of different large-scale language models, requiring them to directly output the final answer without explicitly judging the relevance of the retrieved context. We show that fine-tuning with a mixture of real data and interfering contexts can significantly enhance the robustness of the models to retrieval inaccuracies, while still extracting the correct answer when retrieval is accurate. This suggests that large language models can implicitly handle relevant or irrelevant retrieval contexts in an end-to-end manner by learning only from the supervision of final answers. Introducing the process of explicit relevance judgments may be unnecessary and instead undermine the smoothness of the end-to-end approach.</p>
                </div>
            </div>
        </div>
    </main>

    <footer>
        <p>&copy; 2023 EIT NLP. All rights reserved.</p>
    </footer>
</body>
</html>