<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>EIT NLP - Research</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <header>
        <div class="header-content">
            <nav>
                <ul>
                    <li><h1>EIT NLP</h1></li>
                    <li><a href="index.html">Home</a></li>
                    <li><a href="research.html">Publications</a></li>
                    <li><a href="team.html">Team</a></li>
                    <li><a href="recruitment.html">Recruitment</a></li>
                    <li class="language-switch">
                    </li>
                </ul>
            </nav>
        </div>
    </header>

    <main>
        <h1>Publications</h1>

        <div class="publication-container">
            <div class="publication">
                <img src="path/to/image1.jpg" alt="Fine-Tuning Large Language Models to Translate">
                <div class="publication-content">
                    <h2>Fine-Tuning Large Language Models to Translate: Will a Touch of Noisy Data in Misaligned Languages Suffice?</h2>
                    <p><strong>Authors:</strong> Dawei Zhu, Pinzhen Chen, Miaoran Zhang, Barry Haddow, Xiaoyu Shen, Dietrich Klakow</p>
                    <p><strong>Paper address:</strong> <a href="https://arxiv.org/pdf/2404.14122" target="_blank">https://arxiv.org/pdf/2404.14122</a></p>
                    <p><strong>Code:</strong> <a href="https://github.com/uds-lsv/mt-sft" target="_blank">https://github.com/uds-lsv/mt-sft</a></p>
                    <p><strong>Paper Description:</strong> Traditionally, the success of multilingual machine translation has been attributed to three key factors of training data: large-scale data volume, diverse translation directions, and high quality data. In the current practice of fine-tuning Large Language Models (LLMs) for translation, we revisit the importance of these factors. We find that LLMs exhibit strong translation capabilities when fine-tuned with only 32 pairs of parallel sentences, and that fine-tuning a single translation direction enables multi-directional translation. However, the choice of direction is crucial: fine-tuning using only English as the target language may lead to task misinterpretation, thus hindering translation to non-English languages. Problems can also arise when noisy synthetic data is used as the target language, especially when the target language has been well represented in LLM pre-training. However, it is worth noting that the effect of synthetic data on a language that is poorly represented in the pre-training is milder. Our results suggest that the data volume requirement can be appropriately relaxed when adapting LLMs to translation tasks, but careful consideration is still needed to prevent LLMs from utilizing unintended data biases.</p>
                </div>
            </div>

            <div class="publication reverse">
                <img src="path/to/image2.jpg" alt="The Accuracy Paradox in RLHF">
                <div class="publication-content">
                    <h2>The Accuracy Paradox in RLHF: When Better Reward Models Don't Yield Better Language Models</h2>
                    <p><strong>Author(s):</strong> Yanjun Chen, Dawei Zhu, Yirong Sun, Xinghao Chen, Wei Zhang, Xiaoyu Shen</p>
                    <p><strong>Address:</strong> <a href="https://arxiv.org/abs/2410.06554" target="_blank">https://arxiv.org/abs/2410.06554</a></p>
                    <p><strong>Code:</strong> <a href="https://github.com/EIT-NLP/AccuracyParadox-RLHF" target="_blank">https://github.com/EIT-NLP/AccuracyParadox-RLHF</a></p>
                    <p><strong>Paper Description:</strong> Human feedback reinforcement learning significantly improves natural language processing by training language models to better match human expectations. The strength of the reward model is one of the key factors in this process. This study examines whether stronger reward models necessarily lead to better language model performance. Using experiments with the QA-FEEDBACK dataset and Longformer-based reward models for relevance, factuality, and completeness tasks, this paper reveals a surprising paradox: language models trained with medium-accuracy reward models outperform those trained with high-accuracy reward models. This finding challenges the common belief that stronger reward models always lead to better language model performance, and opens up new directions for future research on the key factors affecting model performance and how to choose appropriate reward models.</p>
                </div>
            </div>

            <div class="publication">
                <img src="path/to/image3.jpg" alt="Unveiling In-Context Learning">
                <div class="publication-content">
                    <h2>Unveiling In-Context Learning: A Coordinate System to Understand Its Working Mechanism</h2>
                    <p><strong>Author(s):</strong> Anhao Zhao, Fanghua Ye, Jinlan Fu, Xiaoyu Shen</p>
                    <p><strong>Address:</strong> <a href="https://arxiv.org/pdf/2407.17011" target="_blank">https://arxiv.org/pdf/2407.17011</a></p>
                    <p><strong>Code:</strong> <a href="https://github.com/EIT-NLP/2D-Coordinate-System-for-ICL" target="_blank">https://github.com/EIT-NLP/2D-Coordinate-System-for-ICL</a></p>
                    <p><strong>Abstract:</strong> Large Language Models (LLMs) have demonstrated a remarkable capability of Context Learning (ICL), however, the working mechanism behind it is still not fully understood. Existing studies have presented two conflicting views on ICL: one emphasizes the importance of similar samples in the demonstration, pointing out the importance of labeling correctness and more examples; the other attributes ICL to the inherent task recognition ability of LLMs, arguing that labeling correctness and the number of examples are not critical. In this study, we propose a two-dimensional coordinate system that unifies these two views in a systematic framework. The framework explains ICL behavior through two orthogonal variables: whether the presentations contain similar samples (perception) and whether the LLMs are able to recognize the task (cognition). We propose peak inverse-order ranking metrics for detecting LLMs' task recognition ability and investigate LLMs' responses to different definitions of similarity. Based on this, we conducted extensive experiments to elucidate the performance of ICL on several representative categorization tasks. Finally, we extend our analysis to generative tasks, showing that this coordinate system is equally effective in explaining the mechanisms by which ICL works in generative tasks.</p>
                </div>
            </div>

            <div class="publication reverse">
                <img src="path/to/image4.jpg" alt="To Preserve or To Compress">
                <div class="publication-content">
                    <h2>To Preserve or To Compress: An In-Depth Study of Connector Selection in Multimodal Large Language Models</h2>
                    <p><strong>Author(s):</strong> Junyan Lin, Haoran Chen, Dawei Zhu, Xiaoyu Shen</p>
                    <p><strong>Address:</strong> <a href="http://arxiv.org/abs/2410.06765" target="_blank">http://arxiv.org/abs/2410.06765</a></p>
                    <p><strong>Code:</strong> <a href="https://github.com/EIT-NLP/Connector-Selection-for-MLLM" target="_blank">https://github.com/EIT-NLP/Connector-Selection-for-MLLM</a></p>
                    <p><strong>Paper Introduction:</strong> In recent years, Multimodal Large Language Models (MLLMs) have attracted a lot of attention from industry and academia. Depending on the fusion location, MLLMs can be categorized into external fusion and internal fusion architectures, among which external fusion architectures are dominant. However, there is still a big controversy about how to build optimal external fusion MLLM architectures, especially the performance of different connectors in different granularity tasks. In this paper, we systematically explore the impact of connectors on MLLM performance. Specifically, we categorize connectors into two types: feature-preserving and feature-compressing. With a unified classification criterion, we classify subtasks from three comprehensive benchmark datasets (MMBench, MME, SEED-Bench) into three task types: coarse-grained perception, fine-grained perception, and inference, and evaluate the performance of connectors from this perspective. The results show that the performance of different connector types varies significantly across tasks, which provides important guidance for MLLM architecture design and advances the understanding and development of MLLM architecture optimization.</p>
                </div>
            </div>

            <div class="publication">
                <img src="path/to/image5.jpg" alt="Deeper Insights Without Updates">
                <div class="publication-content">
                    <h2>Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning</h2>
                    <p><strong>Author(s):</strong> Qingyu Yin, Xuzheng He, Luoao Deng, Chak Tou Leong, Fan Wang, Yanzhao Yan, Xiaoyu Shen, Qiang Zhang</p>
                    <p><strong>Address:</strong> <a href="https://arxiv.org/abs/2410.04691" target="_blank">https://arxiv.org/abs/2410.04691</a></p>
                    <p><strong>Code:</strong> <a href="https://github.com/MikaStars39/ICLvsFinetune" target="_blank">https://github.com/MikaStars39/ICLvsFinetune</a></p>
                    <p><strong>Thesis Statement:</strong> Fine-tuning and In-Context Learning (ICL) are two commonly used methods for injecting task-specific knowledge into large language models. It is often assumed that fine-tuning outperforms ICL with sufficient samples due to its ability to tune the internal parameters of the model based on training data; however, this paper presents a counter-intuitive finding: for implicit pattern tasks, ICL can significantly outperform fine-tuning in capturing these patterns. We constructed multiple datasets containing implicit patterns, such as sequences of answers determined by parity or identifying approximable terms in a computation. We then evaluated the model's understanding of these patterns under fine-tuning and ICL conditions on models with parameter scales ranging from 0.5B to 7B, respectively. The results show that models using ICL can quickly grasp deep patterns and significantly improve accuracy. In contrast, fine-tuning provides limited performance gains despite using thousands of times more training samples than ICL. We also propose a theory of circuit migration from the perspective of mechanical interpretability that explains why ICL performs better on such tasks.</p>
                </div>
            </div>

            <div class="publication reverse">
                <img src="path/to/image6.jpg" alt="LawBench: Benchmarking Legal Knowledge of Large Language Models">
                <div class="publication-content">
                    <h2>LawBench: Benchmarking Legal Knowledge of Large Language Models</h2>
                    <p><strong>Authors:</strong> Zhiwei Fei, Xiaoyu Shen, Dawei Zhu, Fengzhe Zhou, Zhuo Han, Alan Huang, Songyang Zhang, Kai Chen, Zhixin Yin, Zongwen Shen, Jidong Ge, Vincent Ng</p>
                    <p><strong>Paper address:</strong> <a href="https://arxiv.org/pdf/2309.16289" target="_blank">https://arxiv.org/pdf/2309.16289</a></p>
                    <p><strong>Code:</strong> <a href="https://github.com/open-compass/LawBench/" target="_blank">https://github.com/open-compass/LawBench/</a></p>
                    <p><strong>Dissertation Synopsis:</strong> We present LawBench, the first assessment benchmark consisting of 20 tasks designed to evaluate the performance of Large Language Models (LLMs) in processing Chinese law-related tasks. LawBench has been carefully designed to accurately assess the legal competence of LLMs in terms of three cognitive levels corresponding to the widely-accepted Bloom's Cognitive Taxonomy. Using LawBench, we provide a comprehensive assessment of 21 popular LLMs and, for the first time, a comparative analysis of their actual performance, revealing their relative strengths and weaknesses. All data, model predictions, and evaluation code are available from <a href="https://github.com/open-compass/LawBench" target="_blank">https://github.com/open-compass/LawBench</a>.</p>
                </div>
            </div>

            <div class="publication">
                <img src="path/to/image7.jpg" alt="Assessing Implicit Retrieval Robustness of Large Language Models">
                <div class="publication-content">
                    <h2>Assessing "Implicit" Retrieval Robustness of Large Language Models</h2>
                    <p><strong>Author(s):</strong> Xiaoyu Shen, Rexhina Blloshmi, Dawei Zhu, Jiahuan Pei, Wei Zhang</p>
                    <p><strong>Address:</strong> <a href="https://arxiv.org/pdf/2406.18134" target="_blank">https://arxiv.org/pdf/2406.18134</a></p>
                    <p><strong>Paper Introduction:</strong> Retrieval-augmented generation (RAG) has attracted much attention in recent years as a framework for augmenting large language models with external knowledge. However, its effectiveness relies heavily on the retrieval robustness of the model. If the model lacks retrieval robustness, its performance will be limited by the accuracy of the retriever, and when the retrieved context is task-irrelevant, the model performance may drop dramatically. In this paper, we evaluate the "implicit" retrieval robustness of different large-scale language models, requiring them to directly output the final answer without explicitly judging the relevance of the retrieved context. We show that fine-tuning with a mixture of real data and interfering contexts can significantly enhance the robustness of the models to retrieval inaccuracies, while still extracting the correct answer when retrieval is accurate. This suggests that large language models can implicitly handle relevant or irrelevant retrieval contexts in an end-to-end manner by learning only from the supervision of final answers. Introducing the process of explicit relevance judgments may be unnecessary and instead undermine the smoothness of the end-to-end approach.</p>
                </div>
            </div>
        </div>
    </main>

    <footer>
        <p>&copy; 2023 EIT NLP. All rights reserved.</p>
    </footer>

    <script>
        function changeLanguage() {
            var language = document.getElementById("languageSelect").value;
            if (language === "zh") {
                // 切换到中文版本
                document.title = "EIT NLP - 研究";
                document.querySelector('h1').textContent = "出版物";
                // 更新导航栏
                document.querySelector('a[href="index.html"]').textContent = "首页";
                document.querySelector('a[href="research.html"]').textContent = "研究";
                document.querySelector('a[href="team.html"]').textContent = "团队";
                document.querySelector('a[href="recruitment.html"]').textContent = "招聘";
                // 更新页脚
                document.querySelector('footer p').textContent = "© 2023 EIT NLP. 保留所有权利。";
                
                // 更新出版物内容
                const publications = document.querySelectorAll('.publication-content');
                publications.forEach((pub, index) => {
                    const h2 = pub.querySelector('h2');
                    const paragraphs = pub.querySelectorAll('p');
                    
                    // 这里需要为每个出版物准备中文翻译
                    const chineseTitles = [
                        "微调大型语言模型以进行翻译：在不对齐语言中添加少量噪声数据是否足够？",
                        "RLHF中的准确性悖论：为什么更好的奖励模型不一定产生更好的语言模型",
                        "揭示上下文学习：理解其工作机制的坐标系统",
                        "保留还是压缩：多模态大型语言模型中连接器选择的深入研究",
                        "无需更新的深入洞察：上下文学习相对于微调的优势",
                        "LawBench：基准测试大型语言模型的法律知识",
                
                    ];
                    
                    h2.textContent = chineseTitles[index];
                    paragraphs[0].innerHTML = paragraphs[0].innerHTML.replace('Authors:', '作者：');
                    paragraphs[1].innerHTML = paragraphs[1].innerHTML.replace('Paper address:', '论文地址：');
                    paragraphs[2].innerHTML = paragraphs[2].innerHTML.replace('Code:', '代码：');
                    
                    // 这里需要为每个出版物的描述准备中文翻译
                    const chineseDescriptions = [
                        "论文描述：传统上，多语言机器翻译的成功归因于训练数据的三个关键因素：大规模数据量、多样化的翻译方向和高质量数据。在当前微调大型语言模型（LLMs）进行翻译的实践中，我们重新审视了这些因素的重要性。我们发现，LLMs在仅使用32对平行句子进行微调时就表现出强大的翻译能力，并且微调单一翻译方向可以实现多方向翻译。然而，方向的选择至关重要：仅使用英语作为目标语言进行微调可能导致任务误解，从而阻碍了向非英语语言的翻译。当使用噪声合成数据作为目标语言时，特别是当目标语言在LLM预训练中已得到良好表示时，也可能出现问题。然而，值得注意的是，合成数据对预训练中表示不佳的语言的影响较小。我们的结果表明，在将LLMs适应翻译任务时，可以适当放宽数据量要求，但仍需谨慎考虑，以防止LLMs利用非预期的数据偏差。",
                        // 为其他出版物添加中文描述
                    ];
                    
                    const descriptionParagraph = paragraphs[paragraphs.length - 1];
                    descriptionParagraph.innerHTML = descriptionParagraph.innerHTML.replace(/Paper Description:|Paper Introduction:|Abstract:|Thesis Statement:|Dissertation Synopsis:/, '论文描述：');
                    descriptionParagraph.textContent = chineseDescriptions[index];
                });
            } else {
                // 保持英文版本或刷新页面
                location.reload();
            }
        }
    </script>
</body>
</html>