<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>EIT NLP - Home</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <header>
        <div class="header-content">
            <nav>
                <ul>
                    <li><img src="./zhuye//navLogo.png" style="width: 140px;height: 120px;"></li>
                    <li><a href="index.html">Home</a></li>
                    <li><a href="research.html">Publications</a></li>
                    <li><a href="team.html">Team</a></li>
                    <li><a href="recruitment.html">Recruitment</a></li>
                    <li class="language-switch"><a href="index-zh.html">中文</a></li>
                </ul>
            </nav>
        </div>
    </header>

    <main>
        <div class="hero-image">
            <div class="hero-text">
                
            </div>
        </div>



        <section class="news">
            <h2>News</h2>
            <ul style='overflow:scroll; width:98%; height:260px;'>
                <li>
                    <strong>New Paper Released:</strong> "To Preserve or To Compress: An In-Depth Study of Connector Selection in Multimodal Large Language Models"
                    <br>
                    Authors: Junyan Lin, Haoran Chen, Dawei Zhu, Xiaoyu Shen
                    <br>
                    <a href="http://arxiv.org/abs/2410.06765" target="_blank">Paper</a> | 
                    <a href="https://github.com/EIT-NLP/Connector-Selection-for-MLLM" target="_blank">Code Repository</a>
                    <br>
                    This study explores the impact of connectors on the performance of Multimodal Large Language Models (MLLMs), providing important guidance for MLLM architecture design.
                </li>
                <li>
                    <strong>Research Highlight:</strong> "Fine-Tuning Large Language Models to Translate: Will a Touch of Noisy Data in Misaligned Languages Suffice?"
                    <br>
                    Authors: Dawei Zhu, Pinzhen Chen, Miaoran Zhang, Barry Haddow, Xiaoyu Shen, Dietrich Klakow
                    <br>
                    <a href="https://arxiv.org/pdf/2404.14122" target="_blank">Paper</a> | 
                    <a href="https://github.com/uds-lsv/mt-sft" target="_blank">Code Repository</a>
                    <br>
                    This study revisits the importance of training data factors in fine-tuning LLMs for translation, revealing surprising findings about data volume and translation direction requirements.
                </li>
                <li>
                    <strong>Latest Publication:</strong> "The Accuracy Paradox in RLHF: When Better Reward Models Don't Yield Better Language Models"
                    <br>
                    Authors: Yanjun Chen, Dawei Zhu, Yirong Sun, Xinghao Chen, Wei Zhang, Xiaoyu Shen
                    <br>
                    <a href="https://arxiv.org/abs/2410.06554" target="_blank">Paper</a> | 
                    <a href="https://github.com/EIT-NLP/AccuracyParadox-RLHF" target="_blank">Code Repository</a>
                    <br>
                    This research reveals a surprising paradox in human feedback reinforcement learning, challenging the belief that stronger reward models always lead to better language model performance.
                </li>
            </ul>
            <a href="research.html" class="see-more">See more</a>
        </section>
        <section style="position: relative;">
            <h2 style="border-bottom: #000 solid 2px;">About us</h2>
            <p class="about-us-text">We are the NLP group from Eastern Institute of Technology, Ningbo (EIT). Ningbo is a major port city and historical cultural center on China's southeastern coast. The institute is committed to gathering top global talent and aims to establish itself as a high-level, innovative, and international research university. Its goal is to become a world-class academic institution in the short term and lead the development of key strategic disciplines in the country. The institute promotes an academic environment of equality, openness, and freedom, with the ambition of becoming a nonprofit, cutting-edge research institution driving transformative research.
            <p class="about-us-text">At the NLP group, we align with EIT’s mission to drive transformative research. Our work focuses on advancing natural language processing through innovative approaches that enable machines to deeply understand, generate, and reason with human language. We tackle diverse areas such as information retrieval, multi-modal learning, and chain-of-thought reasoning. A core objective is to develop highly efficient algorithms that deliver state-of-the-art performance while minimizing computational and resource demands. We prioritize reducing dependence on large-scale human annotation, speeding up training processes, and cutting down inference costs, ultimately aiming to create robust, resource-efficient models that are accessible and effective across a wide range of applications.
            </p> <a href="https://www.eitech.edu.cn/" target="_blank" style="color: #000; font-size: 16px;font-weight: 700;margin-top: 400px;">learn more</a>
            <p class="about-us-follow">For more updates, follow us on our official WeChat public account: EIT-NLP.</p>
           
            <img src="./zhuye/QRcode.png" style="width: 100px; height: 100px;position: absolute; bottom: -20px; right: 200px;">
        </section>
        <section>
            <h2>Github</h2>
            <p>Our lab focus on open science and will release our code, model and dataset at <a href="https://github.com/eit-nlp" target="_blank" class="button-link">Visit our GitHub</a></p>
        </section>
    </main>

    <footer>
        <p>&copy; 2023 EIT NLP. All rights reserved.</p>
    </footer>

    <script src="script.js"></script>
</body>
</html>
